{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Ï€\n",
    "\n",
    "\n",
    "### Basic properties\n",
    "\n",
    "The probability is just the ratio between the area of the circle and that of the square, i.e:\n",
    "\n",
    "$$\\mathbb{P}[s_i = 4] = \\frac{\\mathcal{A}_{circle}}{\\mathcal{A}_{square}}$$\n",
    "\n",
    "  The mean and variance are easily determined:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "m &= \\mathbb{E}[s_i] = \\frac{\\pi}{4} \\cdot 4 + 0 = \\pi \\\\\n",
    "\\Delta &= \\mathbb{E}[s_i^2] - \\mathbb{E}[s_i]^2 = 4 \\cdot \\pi - \\pi^2 = \\pi(4 - \\pi)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### Estimators\n",
    "\n",
    "The estimators $\\hat{m}$ and $\\hat{\\Delta}$ are both unbiased. In fact:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\mathbb{E}[\\hat{m}] &= \\frac{1}{N} \\cdot N \\cdot \\mathbb{E}[s_i] = m \\qquad \\mathrm{\\blacksquare} \\\\\n",
    "\n",
    "\\mathbb{E}[\\hat{\\Delta}] &= \\frac{1}{N-1} \\sum_{i = 1}^{N} (\\mathbb{E}[s_i^2] - \\mathbb{E}[\\hat{m}^2])\n",
    "= \\frac{N}{N-1} \\{ \\mathbb{E}[s_i^2] - \\frac{1}{N}\\mathbb{E}[s_i^2] - (N - 1)\\mathbb{E}[s_i]^2 \\}\n",
    "= \\frac{N-1}{N-1}(\\mathbb{E}[s_i^2] - \\mathbb{E}[s_i]^2)\n",
    "= \\Delta \\qquad \\mathrm{\\blacksquare}\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    ".\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{m} &= \\frac{1}{N}\\sum_{i=1}^{N}s_i \\\\\n",
    "\\Delta &= \\frac{1}{N-1}\\sum_{i=1}^{N}(s_i^2 - \\hat{m}^2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "are unbiased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Find the lighthouse\n",
    "\n",
    "Since the angular distribution is uniform, considering N total flashes spreaded in the $\\pi$, the number of flashes in an angle $d\\theta$ is: $n(\\theta)=\\frac{Nd\\theta}{\\pi}$, yielding an angular probability density of:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\theta)=\\dfrac{1}{\\pi}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The $x_k$ position on the shore is a function of the angle $\\theta$ through: $x_k=\\beta tan(\\theta_k)$. Each element of lenght $dx$ then depends on $d\\theta$ through:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " dx=\\dfrac{\\beta d\\theta}{cos(\\theta)^{2}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "but \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "cos(\\theta)=\\dfrac{\\beta}{\\sqrt{\\beta^{2}+(x_k-\\alpha)^{2}}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "consequently\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\theta)d\\theta=\\dfrac{1}{\\pi} \\dfrac{dx}{\\dfrac{\\beta}{cos(\\theta)^{2}}}=\\dfrac{\\beta dx}{\\pi(\\beta^{2}+(x_k-\\alpha)^{2})}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(x)=\\dfrac{\\beta}{\\pi(\\beta^{2}+(x_k-\\alpha)^{2})}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical inference & maximum likelihood\n",
    "\n",
    "$$\n",
    "\n",
    "The log-Likelihood function is:\n",
    "\\begin{equation}\n",
    "logL=-\\dfrac{\\sum_{i=1}^{N} x_{i}}{\\lambda}-Nlog(Z(\\lambda))\n",
    "\\end{equation}\n",
    "Setting its first $\\lambda$-derivative equal to 0 we obtain the following relation for the ML estimator $\\lambda^{*}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\dfrac{\\sum_{i=1}^{N} x_{i}}{\\lambda^{*2}}-N\\dfrac{Z'(\\lambda^{*})}{Z(\\lambda^{*})}=0\n",
    "\\end{equation}\n",
    "which can be recast in the followig form:\n",
    "\\begin{equation}\n",
    "\\lambda^{*} +\\dfrac{  e^{1/\\lambda^{*}}  -20 e^{20/\\lambda^{*}} }     {e^{1/\\lambda^{*}} - e^{20/\\lambda^{*}} }     =\\dfrac{\\sum_{i=1}^{N} x_{i}}{N}\n",
    "\\end{equation}\n",
    "\n",
    "now lets call call for simplicity \n",
    "\\begin{equation}\n",
    "\\mu_{*}(\\lambda^{*})=\\lambda^{*} +\\dfrac{  e^{1/\\lambda^{*}}  -20 e^{20/\\lambda^{*}} }     {e^{1/\\lambda^{*}} - e^{20/\\lambda^{*}} }\n",
    "\\end{equation}\n",
    "\n",
    "We know that the sample average $\\dfrac{\\sum_{i=1}^{N} x_{i}}{N}$ is an unbiased estimator of the expected value of a random variable, which in our case will be a function of   $\\lambda_{true}$: $\\mu(\\lambda_{true})$. \n",
    "\n",
    "This means $\\mu_{*}(\\lambda^{*})$ is an unbiased estimator of $\\mu(\\lambda_{true})$.\n",
    "\n",
    "But it turns out that the expected value is:\n",
    "\\begin{equation}\n",
    "\\mu(\\lambda_{true})=\\lambda^{true} +\\dfrac{  e^{1/\\lambda^{true}}  -20 e^{20/\\lambda^{true}} }     {e^{1/\\lambda^{true}} - e^{20/\\lambda^{true}} }\n",
    "\\end{equation}\n",
    "\n",
    "The equality $\\lambda_{*}=\\lambda_{true}$ follows from the fact that $\\mu\\equiv\\mu_{*}$ is a injective function of its argument, being its first derivative always positive.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
